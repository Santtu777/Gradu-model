{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Time Series Forecasting with Multiple Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0. Import libraries and setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Import custom modules\n",
    "from data_loading import load_m4_financial\n",
    "from models import (\n",
    "    ARIMAModel,\n",
    "    HoltWintersModel,\n",
    "    RNNModel,\n",
    "    LSTMModel,\n",
    "    XGBoostModel,\n",
    "    InformerModel,\n",
    "    TFTModel\n",
    ")\n",
    "\n",
    "# Set plotting style\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (12, 6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load M4 Financial data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Loading M4 financial data...\")\n",
    "load_m4_financial()  # This will download the data if it's not already present"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For this notebook, we'll focus on the Monthly financial data\n",
    "from datasetsforecast.m4 import M4\n",
    "m4_info = M4.info()\n",
    "print(f\"M4 dataset info: {m4_info}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load monthly financial data\n",
    "monthly_data = M4.load(directory='./data', group='Monthly', cache=True)\n",
    "print(f\"Dataset shape: {monthly_data['dataset'].shape}\")\n",
    "print(f\"Number of financial time series: {len(monthly_data['dataset'])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract a few financial time series for analysis\n",
    "financial_series = monthly_data['dataset'].iloc[:5]  # First 5 series\n",
    "\n",
    "plt.figure(figsize=(15, 10))\n",
    "for i, series in enumerate(financial_series):\n",
    "    plt.subplot(len(financial_series), 1, i+1)\n",
    "    plt.plot(series)\n",
    "    plt.title(f\"Financial Time Series {i+1}\")\n",
    "    plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Basic statistics of the first 5 financial time series:\")\n",
    "for i, series in enumerate(financial_series):\n",
    "    print(f\"Series {i+1}:\")\n",
    "    print(f\"  Length: {len(series)}\")\n",
    "    print(f\"  Mean: {np.mean(series):.2f}\")\n",
    "    print(f\"  Std: {np.std(series):.2f}\")\n",
    "    print(f\"  Min: {np.min(series):.2f}\")\n",
    "    print(f\"  Max: {np.max(series):.2f}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for seasonality and trend in one example series\n",
    "from statsmodels.tsa.seasonal import seasonal_decompose\n",
    "\n",
    "# Select a single series with sufficient data points\n",
    "example_series = financial_series.iloc[0]\n",
    "if len(example_series) >= 24:  # Need at least 2x the seasonal period\n",
    "    decomposition = seasonal_decompose(example_series, model='multiplicative', period=12)\n",
    "    \n",
    "    plt.figure(figsize=(12, 10))\n",
    "    plt.subplot(4, 1, 1)\n",
    "    plt.plot(example_series)\n",
    "    plt.title('Original')\n",
    "    \n",
    "    plt.subplot(4, 1, 2)\n",
    "    plt.plot(decomposition.trend)\n",
    "    plt.title('Trend')\n",
    "    \n",
    "    plt.subplot(4, 1, 3)\n",
    "    plt.plot(decomposition.seasonal)\n",
    "    plt.title('Seasonality')\n",
    "    \n",
    "    plt.subplot(4, 1, 4)\n",
    "    plt.plot(decomposition.resid)\n",
    "    plt.title('Residuals')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Since we have multiple time series, let's select one for detailed analysis\n",
    "ts_idx = 0  # We'll use the first time series\n",
    "selected_series = financial_series.iloc[ts_idx].dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the forecast horizon\n",
    "forecast_horizon = 12  # For 12 months ahead forecasting\n",
    "\n",
    "# Train-test split\n",
    "train_size = len(selected_series) - forecast_horizon\n",
    "train_data = selected_series[:train_size]\n",
    "test_data = selected_series[train_size:]\n",
    "\n",
    "print(f\"Training data length: {len(train_data)}\")\n",
    "print(f\"Test data length: {len(test_data)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scale data for neural network models\n",
    "scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "train_scaled = scaler.fit_transform(train_data.values.reshape(-1, 1))\n",
    "test_scaled = scaler.transform(test_data.values.reshape(-1, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare windowed dataset for neural networks\n",
    "def create_sequences(data, seq_length):\n",
    "    xs, ys = [], []\n",
    "    for i in range(len(data) - seq_length):\n",
    "        x = data[i:i+seq_length]\n",
    "        y = data[i+seq_length]\n",
    "        xs.append(x)\n",
    "        ys.append(y)\n",
    "    return np.array(xs), np.array(ys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create window size for sequential models\n",
    "window_size = 12  # Use 12 months of data to predict the next month\n",
    "X_train, y_train = create_sequences(train_scaled, window_size)\n",
    "# For testing we'll use the full test data set separately\n",
    "\n",
    "# Reshape for RNN/LSTM models\n",
    "X_train_rnn = X_train.reshape(X_train.shape[0], X_train.shape[1], 1)\n",
    "\n",
    "print(f\"X_train shape for RNN/LSTM: {X_train_rnn.shape}\")\n",
    "print(f\"y_train shape: {y_train.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Model Training & Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to evaluate models\n",
    "def evaluate_forecast(y_true, y_pred):\n",
    "    mae = mean_absolute_error(y_true, y_pred)\n",
    "    rmse = np.sqrt(mean_squared_error(y_true, y_pred))\n",
    "    mape = np.mean(np.abs((y_true - y_pred) / y_true)) * 100\n",
    "    return mae, rmse, mape\n",
    "\n",
    "# Dictionary to store results\n",
    "model_results = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.1 Train and evaluate ARIMA model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nTraining ARIMA model...\")\n",
    "arima_model = ARIMAModel(seasonal=True, m=12)  # Monthly data with yearly seasonality\n",
    "arima_model.fit(train_data)\n",
    "arima_forecast = arima_model.forecast(forecast_horizon)\n",
    "arima_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate\n",
    "mae, rmse, mape = evaluate_forecast(test_data, arima_forecast)\n",
    "model_results['ARIMA'] = {\n",
    "    'mae': mae,\n",
    "    'rmse': rmse,\n",
    "    'mape': mape,\n",
    "    'forecast': arima_forecast\n",
    "}\n",
    "print(f\"ARIMA - MAE: {mae:.2f}, RMSE: {rmse:.2f}, MAPE: {mape:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.2 Train and evaluate Holt-Winters model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nTraining Holt-Winters model...\")\n",
    "hw_model = HoltWintersModel(seasonal='mul', seasonal_periods=12, trend='add')\n",
    "hw_model.fit(train_data)\n",
    "hw_forecast = hw_model.forecast(forecast_horizon)\n",
    "hw_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate\n",
    "mae, rmse, mape = evaluate_forecast(test_data, hw_forecast)\n",
    "model_results['Holt-Winters'] = {\n",
    "    'mae': mae,\n",
    "    'rmse': rmse,\n",
    "    'mape': mape,\n",
    "    'forecast': hw_forecast\n",
    "}\n",
    "print(f\"Holt-Winters - MAE: {mae:.2f}, RMSE: {rmse:.2f}, MAPE: {mape:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.3 Train and evaluate XGBoost model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training XGBoost model...\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'train_data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 18\u001b[39m\n\u001b[32m     15\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m df\n\u001b[32m     17\u001b[39m \u001b[38;5;66;03m# Prepare data for XGBoost\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m18\u001b[39m xgb_data = create_features(\u001b[43mtrain_data\u001b[49m, window=\u001b[32m12\u001b[39m)  \u001b[38;5;66;03m# Use 12 months of lags\u001b[39;00m\n\u001b[32m     19\u001b[39m X_xgb = xgb_data.drop(\u001b[33m'\u001b[39m\u001b[33my\u001b[39m\u001b[33m'\u001b[39m, axis=\u001b[32m1\u001b[39m)\n\u001b[32m     20\u001b[39m y_xgb = xgb_data[\u001b[33m'\u001b[39m\u001b[33my\u001b[39m\u001b[33m'\u001b[39m]\n",
      "\u001b[31mNameError\u001b[39m: name 'train_data' is not defined"
     ]
    }
   ],
   "source": [
    "# We need to prepare the data differently for XGBoost\n",
    "def create_features(series, window=12):\n",
    "    \"\"\"Create features for XGBoost based on lag values\"\"\"\n",
    "    df = pd.DataFrame(series)\n",
    "    df.columns = ['y']\n",
    "    \n",
    "    # Add lag features\n",
    "    for i in range(1, window+1):\n",
    "        df[f'lag_{i}'] = df['y'].shift(i)\n",
    "    \n",
    "    # Drop rows with NaN values\n",
    "    df = df.dropna()\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Prepare data for XGBoost\n",
    "xgb_data = create_features(train_data, window=12)  # Use 12 months of lags\n",
    "X_xgb = xgb_data.drop('y', axis=1)\n",
    "y_xgb = xgb_data['y']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nTraining XGBoost model...\")\n",
    "# Initialize and train XGBoost model\n",
    "xgb_model = XGBoostModel(n_estimators=100, max_depth=3, learning_rate=0.1)\n",
    "xgb_model.fit(X_xgb, y_xgb)\n",
    "xgb_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate forecasts one step at a time\n",
    "xgb_forecast = []\n",
    "last_window = list(train_data[-12:])  # Last 12 months of training data\n",
    "\n",
    "for i in range(forecast_horizon):\n",
    "    # Create features for next prediction\n",
    "    features = np.array(last_window)\n",
    "    features = features.reshape(1, -1)  # Reshape for prediction\n",
    "    \n",
    "    # Make prediction\n",
    "    prediction = xgb_model.predict(features)\n",
    "    xgb_forecast.append(prediction[0])\n",
    "    \n",
    "    # Update window\n",
    "    last_window.pop(0)\n",
    "    last_window.append(prediction[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate\n",
    "mae, rmse, mape = evaluate_forecast(test_data, xgb_forecast)\n",
    "model_results['XGBoost'] = {\n",
    "    'mae': mae,\n",
    "    'rmse': rmse,\n",
    "    'mape': mape,\n",
    "    'forecast': xgb_forecast\n",
    "}\n",
    "print(f\"XGBoost - MAE: {mae:.2f}, RMSE: {rmse:.2f}, MAPE: {mape:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.4 Train and evaluate RNN model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nTraining RNN model...\")\n",
    "# Define input shape for RNN\n",
    "input_shape = (window_size, 1)  # (timesteps, features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize model\n",
    "rnn_model = RNNModel(input_shape=input_shape, hidden_dim=32, output_dim=1)\n",
    "# Train model\n",
    "rnn_history = rnn_model.fit(\n",
    "    X_train_rnn,\n",
    "    y_train,\n",
    "    epochs=50,\n",
    "    batch_size=32,\n",
    "    validation_split=0.2,\n",
    "    verbose=0\n",
    ")\n",
    "rnn_model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate forecasts one step at a time\n",
    "rnn_forecast_scaled = []\n",
    "last_window = train_scaled[-window_size:].reshape(1, window_size, 1)\n",
    "\n",
    "for i in range(forecast_horizon):\n",
    "    # Make prediction\n",
    "    prediction = rnn_model.predict(last_window)\n",
    "    rnn_forecast_scaled.append(prediction[0][0])\n",
    "    \n",
    "    # Update window\n",
    "    last_window = np.append(last_window[:, 1:, :], \n",
    "                           prediction.reshape(1, 1, 1), \n",
    "                           axis=1)\n",
    "\n",
    "# Inverse transform to get original scale\n",
    "rnn_forecast = scaler.inverse_transform(np.array(rnn_forecast_scaled).reshape(-1, 1)).flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate\n",
    "mae, rmse, mape = evaluate_forecast(test_data, rnn_forecast)\n",
    "model_results['RNN'] = {\n",
    "    'mae': mae,\n",
    "    'rmse': rmse,\n",
    "    'mape': mape,\n",
    "    'forecast': rnn_forecast\n",
    "}\n",
    "print(f\"RNN - MAE: {mae:.2f}, RMSE: {rmse:.2f}, MAPE: {mape:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.5 Train and evaluate LSTM model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nTraining LSTM model...\")\n",
    "# Initialize model\n",
    "lstm_model = LSTMModel(input_shape=input_shape, hidden_dim=32, output_dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train model\n",
    "lstm_history = lstm_model.fit(\n",
    "    X_train_rnn,\n",
    "    y_train,\n",
    "    epochs=50,\n",
    "    batch_size=32,\n",
    "    validation_split=0.2,\n",
    "    verbose=0\n",
    ")\n",
    "lstm_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate forecasts one step at a time\n",
    "lstm_forecast_scaled = []\n",
    "last_window = train_scaled[-window_size:].reshape(1, window_size, 1)\n",
    "\n",
    "for i in range(forecast_horizon):\n",
    "    # Make prediction\n",
    "    prediction = lstm_model.predict(last_window)\n",
    "    lstm_forecast_scaled.append(prediction[0][0])\n",
    "    \n",
    "    # Update window\n",
    "    last_window = np.append(last_window[:, 1:, :], \n",
    "                           prediction.reshape(1, 1, 1), \n",
    "                           axis=1)\n",
    "\n",
    "# Inverse transform to get original scale\n",
    "lstm_forecast = scaler.inverse_transform(np.array(lstm_forecast_scaled).reshape(-1, 1)).flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate\n",
    "mae, rmse, mape = evaluate_forecast(test_data, lstm_forecast)\n",
    "model_results['LSTM'] = {\n",
    "    'mae': mae,\n",
    "    'rmse': rmse,\n",
    "    'mape': mape,\n",
    "    'forecast': lstm_forecast\n",
    "}\n",
    "print(f\"LSTM - MAE: {mae:.2f}, RMSE: {rmse:.2f}, MAPE: {mape:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.6 Training and evaluating Informer model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "torch_available = True\n",
    "    \n",
    "# Create a PyTorch dataset for time series\n",
    "class TimeSeriesDataset(Dataset):\n",
    "    def __init__(self, data, window_size):\n",
    "        self.data = torch.FloatTensor(data)\n",
    "        self.window_size = window_size\n",
    "            \n",
    "    def __len__(self):\n",
    "        return len(self.data) - self.window_size\n",
    "            \n",
    "    def __getitem__(self, index):\n",
    "        x = self.data[index:index+self.window_size]\n",
    "        y = self.data[index+self.window_size]\n",
    "        return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define parameters for Informer\n",
    "input_dim = 1  # univariate time series\n",
    "d_model = 64   # embedding dimension\n",
    "d_ff = 128     # feed-forward network dimension\n",
    "n_heads = 4    # number of attention heads\n",
    "e_layers = 2   # number of encoder layers\n",
    "d_layers = 1   # number of decoder layers\n",
    "dropout = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to PyTorch tensors\n",
    "train_tensor = torch.FloatTensor(train_scaled)\n",
    "\n",
    "# Encoder input: Use the last window_size points from training data\n",
    "enc_input = train_tensor[-window_size:].unsqueeze(0).unsqueeze(-1)  # Shape: [1, window_size, 1]\n",
    "\n",
    "# Decoder input: Use a zero tensor as the seed (we'll generate step by step)\n",
    "dec_input = torch.zeros((1, forecast_horizon, 1))  # Shape: [1, forecast_horizon, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the Informer model\n",
    "informer_model = InformerModel(\n",
    "    input_dim=input_dim,\n",
    "    d_model=d_model,\n",
    "    d_ff=d_ff,\n",
    "    n_heads=n_heads,\n",
    "    e_layers=e_layers,\n",
    "    d_layers=d_layers,\n",
    "    dropout=dropout,\n",
    "    out_dim=1\n",
    ")\n",
    "    \n",
    "# Training loop for Informer (normally this would use a proper dataset, but we'll simplify)\n",
    "# Create a dataset from training data\n",
    "train_dataset = TimeSeriesDataset(train_scaled, window_size)\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "# Loss function and optimizer\n",
    "criterion = torch.nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(informer_model.parameters(), lr=0.001)\n",
    "\n",
    "# Training loop\n",
    "epochs = 30\n",
    "informer_model.train()\n",
    "\n",
    "for epoch in range(epochs):\n",
    "        epoch_loss = 0\n",
    "        for batch_x, batch_y in train_loader:\n",
    "            # Add feature dimension if not present\n",
    "            if batch_x.dim() == 2:\n",
    "                batch_x = batch_x.unsqueeze(-1)\n",
    "            \n",
    "            # Create a simple decoder input (we use zeros as placeholder)\n",
    "            batch_size = batch_x.size(0)\n",
    "            decoder_input = torch.zeros((batch_size, 1, 1))\n",
    "            \n",
    "            # Forward pass\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # In a real implementation, we'd use different approaches for \n",
    "            # encoder and decoder inputs. For simplicity, we'll use a hacky approach here.\n",
    "            \n",
    "            # Simulate the Informer forward pass with our model\n",
    "            # Normally: output = informer_model(batch_x, decoder_input)\n",
    "            # But our implementation expects specific encoder/decoder inputs\n",
    "            \n",
    "            # Here we simplify significantly - in practice you'd need to set up \n",
    "            # proper encoder/decoder sequences\n",
    "            enc_in = batch_x\n",
    "            dec_in = decoder_input\n",
    "            \n",
    "            # For our simplified model training\n",
    "            # We'll just predict a single step for now\n",
    "            output = informer_model(enc_in, dec_in)\n",
    "            \n",
    "            # Reshape batch_y to match output dimensions\n",
    "            target = batch_y.unsqueeze(-1).unsqueeze(-1)\n",
    "            \n",
    "            # Calculate loss\n",
    "            loss = criterion(output[:, -1, :], target)\n",
    "            \n",
    "            # Backward pass and optimize\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            epoch_loss += loss.item()\n",
    "        \n",
    "        if epoch % 5 == 0:\n",
    "            print(f\"Epoch {epoch} | Loss: {epoch_loss/len(train_loader):.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.7 Training and evaluating Autoformer model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.8 Training and evaluating TFT model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Model comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare metrics across all models\n",
    "metrics_df = pd.DataFrame({\n",
    "    'MAE': [results['mae'] for model, results in model_results.items()],\n",
    "    'RMSE': [results['rmse'] for model, results in model_results.items()],\n",
    "    'MAPE (%)': [results['mape'] for model, results in model_results.items()]\n",
    "}, index=model_results.keys())\n",
    "\n",
    "print(\"\\nModel Comparison:\")\n",
    "print(metrics_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bar plot for model comparison\n",
    "plt.figure(figsize=(14, 10))\n",
    "\n",
    "# MAE subplot\n",
    "plt.subplot(3, 1, 1)\n",
    "metrics_df['MAE'].plot(kind='bar', color='skyblue')\n",
    "plt.title('Mean Absolute Error (MAE)')\n",
    "plt.grid(axis='y', alpha=0.3)\n",
    "\n",
    "# RMSE subplot\n",
    "plt.subplot(3, 1, 2)\n",
    "metrics_df['RMSE'].plot(kind='bar', color='salmon')\n",
    "plt.title('Root Mean Squared Error (RMSE)')\n",
    "plt.grid(axis='y', alpha=0.3)\n",
    "\n",
    "# MAPE subplot\n",
    "plt.subplot(3, 1, 3)\n",
    "metrics_df['MAPE (%)'].plot(kind='bar', color='lightgreen')\n",
    "plt.title('Mean Absolute Percentage Error (MAPE)')\n",
    "plt.grid(axis='y', alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Visualization of Forecasts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 8))\n",
    "\n",
    "# Plot actual data\n",
    "plt.plot(range(len(train_data)), train_data, 'b-', label='Historical Data')\n",
    "plt.plot(range(len(train_data), len(train_data) + len(test_data)), test_data, 'k-', label='Actual Future')\n",
    "\n",
    "# Plot all model forecasts\n",
    "offset = len(train_data)\n",
    "for model_name, results in model_results.items():\n",
    "    plt.plot(range(offset, offset + forecast_horizon), results['forecast'], '--', label=f'{model_name} Forecast')\n",
    "\n",
    "plt.title('Time Series Forecasting Comparison')\n",
    "plt.xlabel('Time Steps')\n",
    "plt.ylabel('Value')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Conclusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the best model based on MAPE\n",
    "best_model = metrics_df['MAPE (%)'].idxmin()\n",
    "print(f\"\\nThe best performing model is: {best_model}\")\n",
    "print(f\"It achieved a MAPE of {metrics_df.loc[best_model, 'MAPE (%)']: .2f}%\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
